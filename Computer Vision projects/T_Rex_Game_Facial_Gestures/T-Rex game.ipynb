{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import hypot\n",
    "import pyautogui\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 1: Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the weights file\n",
    "model_weights = './res10_300x300_ssd_iter_140000.caffemodel'\n",
    "\n",
    "# path to architecture\n",
    "model_arch = './deploy.prototxt.txt'\n",
    "\n",
    "#load the caffe model\n",
    "net = cv2.dnn.readNetFromCaffe(model_arch,model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector(image, threshold=0.7):\n",
    "    # get the height, width of the image\n",
    "    h,w = image.shape[:2]\n",
    "    # Apply mean substraction and create 4D blob from the image\n",
    "    blob = cv2.dnn.blobFromImage(image,1.0,(300,300),(104.0,177.0,123.0))\n",
    "    # set the new input value for the network\n",
    "    net.setInput(blob)\n",
    "    # run foward path on the input to get the output\n",
    "    faces = net.forward()\n",
    "    # get all the confidence value for all detected faces\n",
    "    prediction_scores = faces[:,:,:,2]\n",
    "    # get the index of the prediction with highest confidence\n",
    "    i = np.argmax(prediction_scores)\n",
    "    # get the face with the highest confidence\n",
    "    face = faces[0,0,i]\n",
    "    # extract the confidence\n",
    "    confidence = face[2]\n",
    "    # if confidence value is greater than the threshold\n",
    "    if confidence> threshold:\n",
    "        # The 4 values at indexes 3-6 are the top-left bottom-right co-ordinates\n",
    "        # scales to range 0-1. The original coordinates can be found by\n",
    "        # multiplying x,y values with the width, height of the  image\n",
    "        box = face[3:7]*np.array([w, h, w, h])\n",
    "        \n",
    "        # the coordinates are the pixel numbers relative to the top left\n",
    "        # corner of the image therfore needs be quantized to int type\n",
    "        (x,y,x1,y1) = box.astype(\"int\")\n",
    "        # draw the bounding box around the face\n",
    "        ted_frame = cv2.rectangle(image.copy(),(x,y),(x1,y1),(0, 255, 255), 2)\n",
    "        output =(ted_frame,(x,y,x1,y1),True,confidence)\n",
    "    else:\n",
    "        output =(image,(),False,0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the face detector\n",
    "# get the video feed from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('face detection', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    # read the frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # flip the frame horizontally\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # detect face in the frame\n",
    "    annotated_frame , coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    # display the frame\n",
    "    cv2.imshow('face detection', annotated_frame)\n",
    "    # break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# when everything is done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 2: Landmarks Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_landmarks(box,image):\n",
    "    # for faster results convert the image to gray-scale\n",
    "    gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # get the coordinates\n",
    "    (x,y,x1,y1) = box\n",
    "    \n",
    "    # perform the detection\n",
    "    shape = predictor(gray_scale, dlib.rectangle(x,y,x1,y1))\n",
    "    \n",
    "    # get the numpy array containing the coordinates of the landmarks\n",
    "    landmarks = shape_to_np(shape)\n",
    "    \n",
    "    # draw the landmarks with circles\n",
    "    for (x,y) in landmarks:\n",
    "        annoted_image = cv2.circle(image,(x,y),2,(0, 127, 255), -1)\n",
    "    \n",
    "    return annoted_image, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper function below converts the shape object returned by the predictor function into a more convenient NumPy array. So the helper function below is being used by the landmark function we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape):\n",
    "    # create an array of shape(68, 2) for storing the landmark coordinates\n",
    "    landmarks = np.zeros((68, 2), dtype='int')\n",
    "    \n",
    "    #write the x,y coordinates of each landmark into the array \n",
    "    for i in range(0, 60):\n",
    "        landmarks[i] = (shape.part(i).x, shape.part(i).y)\n",
    "    \n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the detect_landmark function with realtime feed\n",
    "# get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('landmark', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    # read the frames\n",
    "    ret,frame = cap.read()\n",
    "    \n",
    "    # break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    #flip the frame horizontally\n",
    "    frame = cv2.flip( frame, 1)\n",
    "    \n",
    "    # detect the face\n",
    "    face_image, box_cord, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        # get the landmarks for the face region in the frame\n",
    "        lm_image,landmarks = detect_landmarks(box_cord,frame)\n",
    "    \n",
    "    # display the frame\n",
    "    cv2.imshow('landmark', lm_image)\n",
    "    \n",
    "    # break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "#when done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 3: Jump Control mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mouth_open(landmarks, ar_threshold =0.7):\n",
    "    # calculate the euclidean distance labelled as A,B,C\n",
    "    A = hypot(landmarks[50][0] - landmarks[58][0],landmarks[50][1] - landmarks[58][1])\n",
    "    B = hypot(landmarks[52][0] - landmarks[56][0],landmarks[52][1] - landmarks[56][1])\n",
    "    C = hypot(landmarks[48][0] - landmarks[54][0],landmarks[48][1] - landmarks[54][1])\n",
    "    \n",
    "    # calculate the mouth aspect ratio ,The value of vertical distance A,B is averaged\n",
    "    MAR = (A+B) / (2.0 * C)\n",
    "    \n",
    "    # return true if the value is greater than the threshold\n",
    "    if MAR > ar_threshold:\n",
    "        return True, MAR\n",
    "    else:\n",
    "        return False, MAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the mouth funtion\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cv2.namedWindow('mouth', cv2.WINDOW_NORMAL)\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    if status:\n",
    "        l_image,landmarks = detect_landmarks(box_coords,frame)\n",
    "        mouth_status,_ = is_mouth_open(landmarks,0.55)\n",
    "        cv2.putText(frame,'Is mouth open: {}'.format(mouth_status),(20,20),cv2.FONT_HERSHEY_COMPLEX,0.65,(0, 127, 255), 2)\n",
    "    cv2.imshow('mouth',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 4: Crouch Control Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_proximity(box,image, proximity_threshold = 325):\n",
    "    \n",
    "    # Get the height and width of the face bounding box\n",
    "    face_width =  box[2]-box[0]\n",
    "    face_height = box[3]-box[1]\n",
    "    \n",
    "    # Draw rectangle to guide the user \n",
    "    # Calculate the angle of diagonal using face width, height \n",
    "    theta = np.arctan(face_height/face_width)\n",
    "     \n",
    "    # Use the angle to calculate height, width of the guide rectangle\n",
    "    guide_height = np.sin(theta)*proximity_threshold\n",
    "    guide_width  = np.cos(theta)*proximity_threshold\n",
    "    \n",
    "    # Calculate the mid-point of the guide rectangle/face bounding box\n",
    "    mid_x,mid_y = (box[2]+box[0])/2 , (box[3]+box[1])/2\n",
    "    \n",
    "    # Calculate the coordinates of top-left and bottom-right corners\n",
    "    guide_topleft = int(mid_x-(guide_width/2)), int(mid_y-(guide_height/2))\n",
    "    guide_bottomright = int(mid_x +(guide_width/2)), int(mid_y + (guide_height/2))\n",
    "    \n",
    "    # Draw the guide rectangle\n",
    "    cv2.rectangle(image, guide_topleft, guide_bottomright, (0, 255, 255), 2)\n",
    "    \n",
    "    # Calculate the diagonal distance of the bounding box\n",
    "    diagonal = hypot(face_width, face_height)\n",
    "    \n",
    "    # Return True if distance greater than the threshold\n",
    "    if diagonal > proximity_threshold:\n",
    "        return True, diagonal\n",
    "    else:\n",
    "        return False, diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow('Face proximity', cv2.WINDOW_NORMAL)\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    if status:\n",
    "        is_face_close,_ = face_proximity(box_coords, face_image, proximity_threshold = 325)\n",
    "        cv2.putText(face_image,'Is Face Close: {}'.format(is_face_close),\n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "    cv2.imshow('Face proximity',face_image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    face_image, box_cords, status, conf = face_detector(frame)\n",
    "    if status:\n",
    "        lam_image,landmarks = detect_landmarks(box_cords,frame)\n",
    "        _,mouth_ar = is_mouth_open(landmarks)\n",
    "        _, prox = face_proximity(box_cords, face_image)\n",
    "        # calculate the threshold values\n",
    "        ar_threshold = mouth_ar* 1.4\n",
    "        prox_threshold = prox*1.3\n",
    "        cv2.putText(frame,'Aspect Ratio threshold: {:.2f}'.format(ar_threshold),(20,20),cv2.FONT_HERSHEY_COMPLEX,0.65,(0, 127, 255),2)\n",
    "        cv2.putText(frame,'Aspect Ratio threshold: {:.2f}'.format(prox_threshold),(20,50),cv2.FONT_HERSHEY_COMPLEX,0.65,(0, 127, 255),2)\n",
    "        cv2.imshow('cal',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 6: Automate the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will open a context menu\n",
    "pyautogui.click(button='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# press space bar. this will scroll down the page in some browsers\n",
    "pyautogui.press('space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow('Dino with OpenCV', cv2.WINDOW_NORMAL)\n",
    "# By default each key press is followed by a 0.1 second pause\n",
    "pyautogui.PAUSE = 0.0\n",
    "# The fail-safe triggers an exception in case mouse is moved to corner of the screen\n",
    "#pyautogui.FAILSAFE = False\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    if status:\n",
    "        # Detect landmarks if a face is found\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        # Check if mouth is open\n",
    "        is_open,_ = is_mouth_open(landmarks, ar_threshold)\n",
    "        # If the mouth is open trigger space key Down event to jump\n",
    "        if is_open:\n",
    "            pyautogui.keyDown('space')\n",
    "            mouth_status = 'Open'\n",
    "        else:\n",
    "            # Else the space key is Up\n",
    "            pyautogui.keyUp('space')\n",
    "            mouth_status = 'Closed'\n",
    "        # Display the mouth status on the frame\n",
    "        cv2.putText(frame,'Mouth: {}'.format(mouth_status),\n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "        # Check the proximity of the face\n",
    "        is_closer,_  = face_proximity(box_coords, frame)\n",
    "        if is_closer:\n",
    "            pyautogui.keyDown('down')\n",
    "        else:\n",
    "            pyautogui.keyUp('down')\n",
    "    cv2.imshow('Dino with OpenCV',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}