{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chrome’s Dino game is one of the simplest games you might have ever played. The only two controls that you need to worry about is making the Dino Jump or Crouch in order to avoid obstacles. Normally, you would press the space and down button on the keyboard to make the Dino do that.\n",
    "\n",
    "So all we need to do is programmatically press those buttons, we can easily do that by utilizing the pyautogui library which will allow us to control our keyboard with python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we just need to trigger those controls based on certain actions that I make.\n",
    "\n",
    "So I want to make the Dino jump when I open my mouth, I can do this by detecting the facial landmarks around my lips to determine if my mouth is opened or closed. By using the dlib library I can easily achieve this. I’ll go into more detail on this when I’m implementing this in code.\n",
    "\n",
    "so if the face is closer to the camera the detected size is bigger and the dino should crouch. This way we’re controlling the dino by measuring the proximity between the detected face and the camera. So now you can make the dino crouch by moving your face closer and farther from the camera. I’ll go into more details on this but for now, here are the steps we will be performing in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "\n",
    "    Step 1: Real-time Face Detection\n",
    "    \n",
    "    Step 2: Find the landmarks for the detected face\n",
    "    \n",
    "    Step 3: Build the Jump Control mechanism for the Dino\n",
    "    \n",
    "    Step 4: Build the Crouch Control mechanism\n",
    "    \n",
    "    Step 5: Perform Calibration\n",
    "    \n",
    "    Step 6: Keyboard Automation with PyautoGUI\n",
    "    \n",
    "    Step 7: Build the Final Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import hypot\n",
    "import pyautogui\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 1: Face Detection\n",
    "Now we can use OpenCV’s Haar cascades or Dlib’s HOG-based face detector but instead, I’m going to use a more robust deep learning-based SSD face detector with OpenCV’s DNN module.\n",
    "\n",
    "Initialize The DNN Module:\n",
    "\n",
    "The SSD face detector provided by OpenCV is a Caffe model and you will need two files to do inference with it. A .prototxt file that defines the model architecture and a .caffemodel file that contains the pre-trained weights for the layers in the architecture. Both the Caffe model and .prototxt file will be available in the download folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the weights file\n",
    "model_weights = './res10_300x300_ssd_iter_140000.caffemodel'\n",
    "\n",
    "# path to architecture\n",
    "model_arch = './deploy.prototxt.txt'\n",
    "\n",
    "#load the caffe model\n",
    "net = cv2.dnn.readNetFromCaffe(model_arch,model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create A Face Detection Function:\n",
    "\n",
    "create a function called face_detector() which will take the image as input and detect the faces in the image. Since the Dino has to be controlled using a single input at a time, only one of the detected faces can be used so the bounding rectangle is only returned for the detected face with the highest confidence.\n",
    "\n",
    "We will also need to do some preprocessing steps before we can pass our image to the model, these steps are:\n",
    "\n",
    "Resizing images to 300×300:\n",
    "\n",
    "Applying mean subtraction of values (104, 177, 123):\n",
    "\n",
    "And formating the array structure to a 4D tensor.\n",
    "\n",
    "Fortunately all these things will be taken care of by DNN module’s dnn.blobFromImage() function.\n",
    "\n",
    "After preprocessing we can feed the processed image to the network and then post-process the results. So the model returns a 4-dimensional array, the shape of which in our case is (1, 1, 200, 7). this array contains the confidence score for each detection in the image along with 4 coordinates of the detection scaled down to 0-1 range. For each image, the array also returns 200 detections, However since we are only interested in detecting a single face, we will extract the face with the highest confidence.\n",
    "\n",
    "Finally, the true coordinates for the bounding box rectangle can be then retrieved by multiplying the scaled-down coordinates by the width and height of the original image before resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector(image, threshold=0.7):\n",
    "    # get the height, width of the image\n",
    "    h,w = image.shape[:2]\n",
    "    # Apply mean substraction and create 4D blob from the image\n",
    "    blob = cv2.dnn.blobFromImage(image,1.0,(300,300),(104.0,177.0,123.0))\n",
    "    # set the new input value for the network\n",
    "    net.setInput(blob)\n",
    "    # run foward path on the input to get the output\n",
    "    faces = net.forward()\n",
    "    # get all the confidence value for all detected faces\n",
    "    prediction_scores = faces[:,:,:,2]\n",
    "    # get the index of the prediction with highest confidence\n",
    "    i = np.argmax(prediction_scores)\n",
    "    # get the face with the highest confidence\n",
    "    face = faces[0,0,i]\n",
    "    # extract the confidence\n",
    "    confidence = face[2]\n",
    "    # if confidence value is greater than the threshold\n",
    "    if confidence> threshold:\n",
    "        # The 4 values at indexes 3-6 are the top-left bottom-right co-ordinates\n",
    "        # scales to range 0-1. The original coordinates can be found by\n",
    "        # multiplying x,y values with the width, height of the  image\n",
    "        box = face[3:7]*np.array([w, h, w, h])\n",
    "        \n",
    "        # the coordinates are the pixel numbers relative to the top left\n",
    "        # corner of the image therfore needs be quantized to int type\n",
    "        (x,y,x1,y1) = box.astype(\"int\")\n",
    "        # draw the bounding box around the face\n",
    "        ted_frame = cv2.rectangle(image.copy(),(x,y),(x1,y1),(0, 255, 255), 2)\n",
    "        output =(ted_frame,(x,y,x1,y1),True,confidence)\n",
    "    else:\n",
    "        output =(image,(),False,0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the face detector\n",
    "# get the video feed from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('face detection', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    # read the frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # flip the frame horizontally\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # detect face in the frame\n",
    "    annotated_frame , coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    # display the frame\n",
    "    cv2.imshow('face detection', annotated_frame)\n",
    "    # break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# when everything is done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 2: Landmarks Detection\n",
    "To implement the jump mechanism, we need information about whether the mouth is open or closed. For this, we need to detect facial landmarks so we can determine the position of upper and lower lips.\n",
    "\n",
    "By using Dlib’s 68 landmark detector, we’ll be able to detect below 68 landmarks on the face.\n",
    "\n",
    "The landmark detection model is an implementation of the paper One Millisecond Face Alignment with an Ensemble of Regression Trees by Vahid Kazemi and Josephine Sullivan(2014).\n",
    "\n",
    "To initialize the landmark detector, you will use dlib.shape_predictor() function, which will load the pre-trained landmark detector from the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create the detect_landmarks() function\n",
    "\n",
    "create a function called detect_landmarks() which takes in the coordinates of the detected face returned by the face_detector() method and then detects landmarks and returns them, while also annotating the image with circles on landmark positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_landmarks(box,image):\n",
    "    # for faster results convert the image to gray-scale\n",
    "    gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # get the coordinates\n",
    "    (x,y,x1,y1) = box\n",
    "    \n",
    "    # perform the detection\n",
    "    shape = predictor(gray_scale, dlib.rectangle(x,y,x1,y1))\n",
    "    \n",
    "    # get the numpy array containing the coordinates of the landmarks\n",
    "    landmarks = shape_to_np(shape)\n",
    "    \n",
    "    # draw the landmarks with circles\n",
    "    for (x,y) in landmarks:\n",
    "        annoted_image = cv2.circle(image,(x,y),2,(0, 127, 255), -1)\n",
    "    \n",
    "    return annoted_image, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper function below converts the shape object returned by the predictor function into a more convenient NumPy array. So the helper function below is being used by the landmark function we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape):\n",
    "    # create an array of shape(68, 2) for storing the landmark coordinates\n",
    "    landmarks = np.zeros((68, 2), dtype='int')\n",
    "    \n",
    "    #write the x,y coordinates of each landmark into the array \n",
    "    for i in range(0, 60):\n",
    "        landmarks[i] = (shape.part(i).x, shape.part(i).y)\n",
    "    \n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the detect_landmark function with realtime feed\n",
    "# get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('landmark', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    # read the frames\n",
    "    ret,frame = cap.read()\n",
    "    \n",
    "    # break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    #flip the frame horizontally\n",
    "    frame = cv2.flip( frame, 1)\n",
    "    \n",
    "    # detect the face\n",
    "    face_image, box_cord, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        # get the landmarks for the face region in the frame\n",
    "        lm_image,landmarks = detect_landmarks(box_cord,frame)\n",
    "    \n",
    "    # display the frame\n",
    "    cv2.imshow('landmark', lm_image)\n",
    "    \n",
    "    # break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "#when done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 3: Jump Control mechanism\n",
    "In this step, jump control mechanism is implemented\n",
    "\n",
    "The jump control mechanism that we will use simply utilizes the euclidean distance between a pair of landmark points indicated below to calculate a ratio of mouth height to its width. Using a threshold value for comparison the mouth can then be evaluated as being close or open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mouth_open(landmarks, ar_threshold):\n",
    "    # calculate the euclidean distance labelled as A,B,C\n",
    "    A = hypot(landmarks[50][0] - landmarks[58][0],landmarks[50][1] - landmarks[58][1])\n",
    "    B = hypot(landmarks[52][0] - landmarks[56][0],landmarks[52][1] - landmarks[56][1])\n",
    "    C = hypot(landmarks[48][0] - landmarks[54][0],landmarks[48][1] - landmarks[54][1])\n",
    "    \n",
    "    # calculate the mouth aspect ratio ,The value of vertical distance A,B is averaged\n",
    "    MAR = (A+B) / (2.0 * C)\n",
    "    \n",
    "    # return true if the value is greater than the threshold\n",
    "    if MAR > ar_threshold:\n",
    "        return True, MAR\n",
    "    else:\n",
    "        return False, MAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the mouth funtion\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cv2.namedWindow('mouth', cv2.WINDOW_NORMAL)\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    if status:\n",
    "        l_image,landmarks = detect_landmarks(box_coords,frame)\n",
    "        mouth_status,_ = is_mouth_open(landmarks,0.55)\n",
    "        cv2.putText(frame,'Is mouth open: {}'.format(mouth_status),(20,20),cv2.FONT_HERSHEY_COMPLEX,0.65,(0, 127, 255), 2)\n",
    "    cv2.imshow('mouth',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 4: Crouch Control Mechanism\n",
    "The Crouch control mechanism will utilize the euclidean distance between the top-left corner and bottom-right corner of the face bounding box. When the face is near the camera the distance will be greater, and when the face is close enough a key down event will be triggered causing the Dino to crouch.\n",
    "\n",
    "Jump control mechanism using distance from camera\n",
    "The face_proximity function below calculates the diagonal distance and compares it with the proximity_threshold to return either True or False. Also, the coordinates of a rectangle are calculated relative to the face. This rectangle guides the user on how close they need to get to the camera to trigger the crouch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdbdasf\n",
      "2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-c469029ea9ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m  \u001b[1;31m# Write code here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-c469029ea9ba>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mblood\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mblood\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "def face_proximity(box,image,prox_thresh=250):\n",
    "    # get height and width from the bounding box\n",
    "    face_width = box[2] - box[0]\n",
    "    face_height = box[3]  - box[1]\n",
    "    \n",
    "    # draw rectangle to guide the user \n",
    "    #calculate the angle of diagonal using face width and height\n",
    "    theta = np.arctan(face_height/face_width)\n",
    "    \n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
